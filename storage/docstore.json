{"docstore/metadata": {"77044eea-f7b2-4e9e-b0f6-ddf5df487027": {"doc_hash": "61833a9db3514fc973c192e415ebad5aff33a1ee628df1ec40457383a081c0cc"}, "089a055f-e77f-4992-bea9-0af367409916": {"doc_hash": "6918f646750a546f7a6181cf3a9d9b5999e280f7a07edeee587923d754bd5222", "ref_doc_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027"}, "bb2df435-b399-4d1b-a5b6-8d5728263e1b": {"doc_hash": "1eb6b37826f71b182a00c6b854bbf0f8da507aba74559ff774bc7a400d0227b8", "ref_doc_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027"}, "48f8d124-abbf-4c13-80a3-e62f895659df": {"doc_hash": "d5e3e0a3509d8bcb7be2de1e124c4c63cc965ba986b6d1dbe886eb07816f2044", "ref_doc_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027"}, "d0673c31-8b77-4c10-9d9a-9c0df2250f5e": {"doc_hash": "c3342e69f0a6e0eef60168305fe494fa123de6db0688165245491bc365ec0c4a", "ref_doc_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027"}, "8bf511ad-8dc2-4c2a-9aa4-8917a8071e7d": {"doc_hash": "2c1967704a0bbbb91581301ba1e2abffcf07059b8c7329718392e97fa99888a5", "ref_doc_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027"}}, "docstore/data": {"089a055f-e77f-4992-bea9-0af367409916": {"__data__": {"id_": "089a055f-e77f-4992-bea9-0af367409916", "embedding": null, "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027", "node_type": "4", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "61833a9db3514fc973c192e415ebad5aff33a1ee628df1ec40457383a081c0cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb2df435-b399-4d1b-a5b6-8d5728263e1b", "node_type": "1", "metadata": {}, "hash": "323af9e3c608dde533cf0acea67c73ea2cc2b4e4d6954991e0d133b816512d1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large language models (LLMs), such as those offered in OpenAI\u2019s ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for natural language processing (NLP). Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text. For example, previous generations of language models could not write an email from a list of keywords\u2014a task that is trivial for contemporary LLMs.\r\n\r\nLLMs have remarkable capabilities to understand, generate, and interpret human language. However, it\u2019s important to clarify that when we say language models \u201cunderstand,\u201d we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.\r\n\r\nEnabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.\r\n\r\nAnother important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks.\r\n\r\nThe success of LLMs can be attributed to the transformer architecture that underpins many LLMs and the vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to encode manually.\r\n\r\nThis shift toward implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language.\r\n\r\nThe following discussion sets a foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture, step by step in code.\r\n\r\n1.1 What is an LLM?\r\nAn LLM is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet.\r\n\r\nThe \u201clarge\u201d in \u201clarge language model\u201d refers to both the model\u2019s size in terms of parameters and the immense dataset on which it\u2019s trained. Models like this often have tens or even hundreds of billions of parameters, which are the adjustable weights in the network that are optimized during training to predict the next word in a sequence. Next-word prediction is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text. It is a very simple task, and so it is surprising to many researchers that it can produce such capable models. In later chapters, we will discuss and implement the next-word training procedure step by step.\r\n\r\nLLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.\r\n\r\nSince LLMs are capable of generating text, they are also often referred to as a form of generative artificial intelligence, often abbreviated as generative AI or GenAI. As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring human-like intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning.\r\nThe algorithms used to implement AI are the focus of the field of machine learning. Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed. To illustrate this, imagine a spam filter as a practical application of machine learning. Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails. By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb2df435-b399-4d1b-a5b6-8d5728263e1b": {"__data__": {"id_": "bb2df435-b399-4d1b-a5b6-8d5728263e1b", "embedding": null, "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027", "node_type": "4", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "61833a9db3514fc973c192e415ebad5aff33a1ee628df1ec40457383a081c0cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "089a055f-e77f-4992-bea9-0af367409916", "node_type": "1", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "6918f646750a546f7a6181cf3a9d9b5999e280f7a07edeee587923d754bd5222", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48f8d124-abbf-4c13-80a3-e62f895659df", "node_type": "1", "metadata": {}, "hash": "30a1e6171038e0216e07cbed876528b329d676b9828973f14f3efaefa7634095", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring human-like intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning.\r\nThe algorithms used to implement AI are the focus of the field of machine learning. Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed. To illustrate this, imagine a spam filter as a practical application of machine learning. Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails. By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam.\r\n\r\nAs illustrated in figure 1.1, deep learning is a subset of machine learning that focuses on utilizing neural networks with three or more layers (also called deep neural networks) to model complex patterns and abstractions in data. In contrast to deep learning, traditional machine learning requires manual feature extraction. This means that human experts need to identify and select the most relevant features for the model.\r\n\r\nWhile the field of AI is now dominated by machine learning and deep learning, it also includes other approaches\u2014for example, using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning.\r\n\r\nReturning to the spam classification example, in traditional machine learning, human experts might manually extract features from email text such as the frequency of certain trigger words (for example, \u201cprize,\u201d \u201cwin,\u201d \u201cfree\u201d), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links. This dataset, created based on these expert-defined features, would then be used to train the model. In contrast to traditional machine learning, deep learning does not require manual feature extraction. This means that human experts do not need to identify and select the most relevant features for a deep learning model. (However, both traditional machine learning and deep learning for spam classification still require the collection of labels, such as spam or non-spam, which need to be gathered either by an expert or users.)\r\n\r\nLet\u2019s look at some of the problems LLMs can solve today, the challenges that LLMs address, and the general LLM architecture we will implement later.\r\n\r\n1.2 Applications of LLMs\r\nOwing to their advanced capabilities to parse and understand unstructured text data, LLMs have a broad range of applications across various domains. Today, LLMs are employed for machine translation, generation of novel texts (see figure 1.2), sentiment analysis, text summarization, and many other tasks. LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code.\r\n\r\nLLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI\u2019s ChatGPT or Google\u2019s Gemini (formerly called Bard), which can answer user queries and augment traditional search engines such as Google Search or Microsoft Bing.\r\n\r\nMoreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law. This includes sifting through documents, summarizing lengthy passages, and answering technical questions.\r\n\r\nIn short, LLMs are invaluable for automating almost any task that involves parsing and generating text. Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it\u2019s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible.\r\n\r\nWe will focus on understanding how LLMs work from the ground up, coding an LLM that can generate text. You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions to summarizing text, translating text into different languages, and more. In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step.\r\n\r\n1.3 Stages of building and using LLMs\r\nWhy should you build your own LLM? Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations. Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks.\r\n\r\nNOTE \u2003Most LLMs today are implemented using the PyTorch deep learning library, which is what we will use.", "mimetype": "text/plain", "start_char_idx": 4229, "end_char_idx": 9044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48f8d124-abbf-4c13-80a3-e62f895659df": {"__data__": {"id_": "48f8d124-abbf-4c13-80a3-e62f895659df", "embedding": null, "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027", "node_type": "4", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "61833a9db3514fc973c192e415ebad5aff33a1ee628df1ec40457383a081c0cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb2df435-b399-4d1b-a5b6-8d5728263e1b", "node_type": "1", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "1eb6b37826f71b182a00c6b854bbf0f8da507aba74559ff774bc7a400d0227b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0673c31-8b77-4c10-9d9a-9c0df2250f5e", "node_type": "1", "metadata": {}, "hash": "9e1d75a9f62f8016d13ca23f3ac893ada0d60e7a3e0c17afaf5c1c5ca59caf92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will focus on understanding how LLMs work from the ground up, coding an LLM that can generate text. You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions to summarizing text, translating text into different languages, and more. In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step.\r\n\r\n1.3 Stages of building and using LLMs\r\nWhy should you build your own LLM? Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations. Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks.\r\n\r\nNOTE \u2003Most LLMs today are implemented using the PyTorch deep learning library, which is what we will use. Readers can find a comprehensive introduction to PyTorch in appendix A.\r\n\r\nResearch has shown that when it comes to modeling performance, custom-built LLMs\u2014those tailored for specific tasks or domains\u2014can outperform general-purpose LLMs, such as ChatGPT, which are designed for a wide array of applications. Examples of these include BloombergGPT (specialized for finance) and LLMs tailored for medical question answering (see appendix B for details).\r\n\r\nUsing custom-built LLMs offers several advantages, particularly regarding data privacy. For instance, companies may prefer not to share sensitive data with third-party LLM providers like OpenAI due to confidentiality concerns. Additionally, developing smaller, custom LLMs enables deployment directly on customer devices, such as laptops and smartphones, which is something companies like Apple are currently exploring. This local implementation can significantly decrease latency and reduce server-related costs. Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates and modifications to the model as needed.\r\n\r\nThe first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text. Here, \u201craw\u201d refers to the fact that this data is just regular text without any labeling information. (Filtering may be applied, such as removing formatting characters or documents in unknown languages.)\r\n\r\nNOTE \u2003Readers with a background in machine learning may note that labeling information is typically required for traditional machine learning models and deep neural networks trained via the conventional supervised learning paradigm. This is not the case for the pretraining stage of LLMs. In this phase, LLMs use self-supervised learning, where the model generates its own labels from the input data.\r\n\r\nThis first training stage of an LLM is also known as pretraining, creating an initial pretrained LLM, often called a base or foundation model. A typical example of such a model is the GPT-3 model (the precursor of the original model offered in ChatGPT). This model is capable of text completion\u2014that is, finishing a half-written sentence provided by a user. It also has limited few-shot capabilities, which means it can learn to perform new tasks based on only a few examples instead of needing extensive training data.\r\n\r\nAfter obtaining a pretrained LLM by training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as fine-tuning.\r\n\r\nThe two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning. In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text. In classification fine-tuning, the labeled dataset consists of texts and associated class labels\u2014for example, emails associated with \u201cspam\u201d and \u201cnot spam\u201d labels.\r\n\r\nWe will cover code implementations for pretraining and fine-tuning an LLM, and we will delve deeper into the specifics of both instruction and classification fine-tuning after pretraining a base LLM.\r\n\r\n1.4 Introducing the transformer architecture\r\nMost modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper \u201cAttention Is All You Need\u201d (https://arxiv.org/abs/1706.03762). To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French. A simplified version of the transformer architecture is depicted in figure 1.4.\r\n\r\nThe transformer architecture consists of two submodules: an encoder and a decoder. The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the output text.", "mimetype": "text/plain", "start_char_idx": 8199, "end_char_idx": 13075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0673c31-8b77-4c10-9d9a-9c0df2250f5e": {"__data__": {"id_": "d0673c31-8b77-4c10-9d9a-9c0df2250f5e", "embedding": null, "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027", "node_type": "4", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "61833a9db3514fc973c192e415ebad5aff33a1ee628df1ec40457383a081c0cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48f8d124-abbf-4c13-80a3-e62f895659df", "node_type": "1", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "d5e3e0a3509d8bcb7be2de1e124c4c63cc965ba986b6d1dbe886eb07816f2044", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bf511ad-8dc2-4c2a-9aa4-8917a8071e7d", "node_type": "1", "metadata": {}, "hash": "353da27a51cf70be025b767aa71a5c3805502411a5532f2c33d272768cee89bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.4 Introducing the transformer architecture\r\nMost modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper \u201cAttention Is All You Need\u201d (https://arxiv.org/abs/1706.03762). To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French. A simplified version of the transformer architecture is depicted in figure 1.4.\r\n\r\nThe transformer architecture consists of two submodules: an encoder and a decoder. The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the output text. In a translation task, for example, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language. Both the encoder and decoder consist of many layers connected by a so-called self-attention mechanism. You may have many questions regarding how the inputs are preprocessed and encoded. These will be addressed in a step-by-step implementation in subsequent chapters.\r\n\r\nA key component of transformers and LLMs is the self-attention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output. However, due to its complexity, we will defer further explanation to chapter 3, where we will discuss and implement it step by step.\r\n\r\nLater variants of the transformer architecture, such as BERT (short for bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks. If interested, refer to appendix B for further reading suggestions.\r\n\r\nBERT, which is built upon the original transformer\u2019s encoder submodule, differs in its training approach from GPT. While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence, as shown in figure 1.5. This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction and document categorization. As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content.\r\n\r\nGPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts. This includes machine translation, text summarization, fiction writing, writing computer code, and more.\r\n\r\nGPT models, primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities. These models are adept at executing both zero-shot and few-shot learning tasks. Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples. On the other hand, few-shot learning involves learning from a minimal number of examples the user provides as input, as shown in figure 1.6.\r\n\r\nTransformers vs. LLMs\r\nToday\u2019s LLMs are based on the transformer architecture. Hence, transformers and LLMs are terms that are often used synonymously in the literature. However, note that not all transformers are LLMs since transformers can also be used for computer vision. Also, not all LLMs are transformers, as there are LLMs based on recurrent and convolutional architectures. The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs. Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen. For simplicity, I use the term \u201cLLM\u201d to refer to transformer-based LLMs similar to GPT. (Interested readers can find literature references describing these architectures in appendix B.)\r\n\r\n1.5 Utilizing large datasets\r\nThe large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages. To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT.", "mimetype": "text/plain", "start_char_idx": 12263, "end_char_idx": 16994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bf511ad-8dc2-4c2a-9aa4-8917a8071e7d": {"__data__": {"id_": "8bf511ad-8dc2-4c2a-9aa4-8917a8071e7d", "embedding": null, "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77044eea-f7b2-4e9e-b0f6-ddf5df487027", "node_type": "4", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "61833a9db3514fc973c192e415ebad5aff33a1ee628df1ec40457383a081c0cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0673c31-8b77-4c10-9d9a-9c0df2250f5e", "node_type": "1", "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}, "hash": "c3342e69f0a6e0eef60168305fe494fa123de6db0688165245491bc365ec0c4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs. Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen. For simplicity, I use the term \u201cLLM\u201d to refer to transformer-based LLMs similar to GPT. (Interested readers can find literature references describing these architectures in appendix B.)\r\n\r\n1.5 Utilizing large datasets\r\nThe large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages. To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT.\r\n\r\nTable 1.1 The pretraining dataset of the popular GPT-3 LLM\r\nDataset name\r\nDataset description\r\nNumber of tokens\r\nProportion in training data\r\nCommonCrawl (filtered)\r\nWeb crawl data\r\n410 billion\r\n60%\r\nWebText2\r\nWeb crawl data\r\n19 billion\r\n22%\r\nBooks1\r\nInternet-based book corpus\r\n12 billion\r\n8%\r\nBooks2\r\nInternet-based book corpus\r\n55 billion\r\n8%\r\nWikipedia\r\nHigh-quality text\r\n3 billion\r\n3%\r\nTable 1.1 reports the number of tokens, where a token is a unit of text that a model reads and the number of tokens in a dataset is roughly equivalent to the number of words and punctuation characters in the text. Chapter 2 addresses tokenization, the process of converting text into tokens.\r\n\r\nThe main takeaway is that the scale and diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, and context\u2014even some requiring general knowledge.\r\n\r\nGPT-3 dataset details\r\nTable 1.1 displays the dataset used for GPT-3. The proportions column in the table sums up to 100% of the sampled data, adjusted for rounding errors. Although the subsets in the Number of Tokens column total 499 billion, the model was trained on only 300 billion tokens. The authors of the GPT-3 paper did not specify why the model was not trained on all 499 billion tokens.\r\n\r\nFor context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage. In comparison, later iterations of models like GPT-3, such as Meta\u2019s LLaMA, have expanded their training scope to include additional data sources like Arxiv research papers (92 GB) and StackExchange\u2019s code-related Q&As (78 GB).\r\n\r\nThe authors of the GPT-3 paper did not share the training dataset, but a comparable dataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024 (https://arxiv.org/abs/2402.00159). However, the collection may contain copyrighted works, and the exact usage terms may depend on the intended use case and country.\r\n\r\nThe pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models. Pretraining LLMs requires access to significant resources and is very expensive. For example, the GPT-3 pretraining cost is estimated to be $4.6 million in terms of cloud computing credits (https://mng.bz/VxEW).\r\n\r\nThe good news is that many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, and edit texts that were not part of the training data. Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing the computational resources needed and improving performance.\r\n\r\nWe will implement the code for pretraining and use it to pretrain an LLM for educational purposes. All computations are executable on consumer hardware. After implementing the pretraining code, we will learn how to reuse openly available model weights and load them into the architecture we will implement, allowing us to skip the expensive pretraining stage when we fine-tune our LLM.", "mimetype": "text/plain", "start_char_idx": 16114, "end_char_idx": 20190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"77044eea-f7b2-4e9e-b0f6-ddf5df487027": {"node_ids": ["089a055f-e77f-4992-bea9-0af367409916", "bb2df435-b399-4d1b-a5b6-8d5728263e1b", "48f8d124-abbf-4c13-80a3-e62f895659df", "d0673c31-8b77-4c10-9d9a-9c0df2250f5e", "8bf511ad-8dc2-4c2a-9aa4-8917a8071e7d"], "metadata": {"file_path": "f:\\AI\\RAG\\QASystems\\Data\\TextFile1.txt", "file_name": "TextFile1.txt", "file_type": "text/plain", "file_size": 20276, "creation_date": "2025-04-23", "last_modified_date": "2025-04-23"}}}}